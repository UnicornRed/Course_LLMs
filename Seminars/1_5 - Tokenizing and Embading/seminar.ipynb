{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63fdd2d0",
   "metadata": {},
   "source": [
    "# Токенизация\n",
    "\n",
    "Текст является очень сложным объектом, который сложно обработать как единый объект, так как порядок слов может измениться, а смысл остаться тем же. К тому же существует слишком много различных текстов различной длины, поэтому для модели может быть очень сложно найти закономерности во всех них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac17649",
   "metadata": {},
   "source": [
    "Токен — минимальная единица текста, которую модель может «видеть» и с которой работает:\n",
    "- **Слово** (англ.)  \n",
    "- **Фраза/промежуток** в некоторых языках (напр., китайский)  \n",
    "- **Субсловный токен** (часть слова: `##ing`, `un`, `##pre`)  \n",
    "\n",
    "Токенизация — получение из текста набора токенов для использования в дальнейшем в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a759ce",
   "metadata": {},
   "source": [
    "## Цели токенизации\n",
    "\n",
    "1. Уменьшить размер словаря: сокращение количества уникальных элементов (например, объединяя редкие слова в подслова);\n",
    "\n",
    "2. Позволить модели работать с неизвестными словами: через субсловные токены (`<unk>` → «неизвестное») можно восстановить слово из частей;\n",
    "\n",
    "3. Упростить дальнейший шаг – эмбеддинги: токен‑индекс -> вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055836b",
   "metadata": {},
   "source": [
    "## Классы подходов к токенизации\n",
    "\n",
    "| Подход | Как работает | Примеры библиотек / моделей |\n",
    "|--------|--------------|-----------------------------|\n",
    "| **Пробел + пунктуация** (Whitespace Tokenizer) | Делит строку по пробелам, оставляя знаки препинания как отдельные токены | `nltk.word_tokenize`, `spaCy` с моделью `xx_ent_wiki_sm` |\n",
    "| **Byte‑Pair Encoding (BPE)** | Инициализирует словарь символов, последовательно объединяет наиболее частые пары символов → субсловные токены | GPT‑2, GPT‑3, T5 |\n",
    "| **WordPiece** | Подобно BPE, но с фиксированным порогом вероятности/частоты; используется в BERT, RoBERTa | `transformers.BertTokenizer` |\n",
    "| **SentencePiece** | Делит на уровне байтов, не требует предварительного разделения текста по пробелам; может работать и для языков без пробелов (китайский) | T5, MT‑models от Google |\n",
    "| **Morphological Tokenizer** | Использует морфологический анализатор для разложения слова на корень + суффикс | `pymorphy2`, `spaCy` (русский) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8590a7",
   "metadata": {},
   "source": [
    "## Упражнение\n",
    "\n",
    "Реализовать токенизацию \"по пробелам\" на корпусе текста и попробовать закодировать и раскодировать фразу, которой не было в этом корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ed533",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Какая-то большая строка\",\n",
    "    \"Или набор строк\"\n",
    "]\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        pass # Your code here\n",
    "\n",
    "    # Здесь токенизатор тренируется на корпусе\n",
    "    def train(self, corpus):\n",
    "        pass #Your code here\n",
    "\n",
    "    # Здесь токенизатор кодирует вашу фразу\n",
    "    def encode(self, phrase):\n",
    "        pass # Your code here\n",
    "\n",
    "    # А здесь может раскодировать обратно\n",
    "    def decode(self, phrase):\n",
    "        pass # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6395f4d",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "Реализовать Bite Pair Encoding (BPE) по аналогии с упражнением. Сравнить с токенизатором из `tokenizers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fbce5",
   "metadata": {},
   "source": [
    "# Эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdad1fd",
   "metadata": {},
   "source": [
    "Мы перешли от громадного словаря всех слов в языке к чуть более компактному множеству токенов (десятки тысяч против сотен). Однако полученные токены - это ничто иное как категориальные признаки, которые невозможно обрабатывать с помощью компьютера без преобразования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68212fef",
   "metadata": {},
   "source": [
    "## Зачем нужны эмбеддинги?\n",
    "\n",
    "1. Перевод слов (категориальных признаков) в числовые вектора (количественные признаки);\n",
    "\n",
    "2. Учёт семантики: можно сравнивать вектора-слова и даже складывать;\n",
    "\n",
    "3. Уменьшение размерности: one-hot кодирование катигориальных фичей крайне затратно по памяти, а размер эмбеддингов обычно не превышает 700-800."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9aa19",
   "metadata": {},
   "source": [
    "## Варианты создания эмбеддингов\n",
    "\n",
    "1. Статические эмбеддинги: одно слово - один вектор. Word2Vec - самый распространённый алгоритм;\n",
    "\n",
    "2. Котекстные эмбеддинги: в зависимости от контекста даёт разные эмбеддинги. Обычно являются нейронными сетями-трансформерами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedd07c",
   "metadata": {},
   "source": [
    "Объяснение эмбеддингов от [Lena Voita](https://lena-voita.github.io/nlp_course/word_embeddings.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc383fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность эмбеддинга: torch.Size([1, 768])\n",
      "Эмбеддинг предложения:\n",
      " tensor([ 3.0628e-01, -3.5940e-01, -1.3252e-01,  1.2935e-02, -2.0835e-01,\n",
      "         1.3226e-01,  2.1741e-02, -3.5004e-02, -2.0792e-01, -1.2197e-01,\n",
      "         1.6504e-02, -1.6700e-02, -4.8665e-01, -9.4354e-02, -7.7113e-01,\n",
      "        -1.5182e-01, -2.3959e-02,  4.3575e-01, -2.6022e-01,  4.1086e-01,\n",
      "         7.7340e-02,  9.7452e-02, -7.9095e-02,  1.8498e-01, -8.0374e-02,\n",
      "        -1.4321e-01,  6.3294e-02,  3.1726e-01,  5.7587e-01,  1.1511e-01,\n",
      "         2.0638e-01, -2.0034e-02, -2.6985e-03, -1.4731e-01,  2.5237e-01,\n",
      "        -1.4301e-01, -1.9171e+00, -2.3978e-01,  4.9484e-02, -3.5315e-02,\n",
      "         8.4558e-02, -6.4724e-02,  1.1235e-01, -1.2939e-01, -3.1030e-02,\n",
      "         1.2212e+00,  6.8097e-02, -1.1456e-02,  1.0866e+00, -3.0592e-02,\n",
      "         9.9322e-02, -7.5821e-01,  1.7140e-01, -1.3419e+00, -4.9655e-02,\n",
      "         1.6684e-01, -9.2096e-02, -3.2680e-01, -2.0072e-01,  4.6293e-02,\n",
      "         2.1990e-01, -5.7322e-02,  2.8037e-01, -6.4665e-01, -2.6249e-02,\n",
      "         1.3564e-02,  2.3228e-01, -1.9951e-02, -3.8059e-01,  3.4496e-01,\n",
      "        -2.1131e-01,  5.0271e-02,  2.3454e-01,  1.1456e-01, -3.9597e-02,\n",
      "         2.7441e-02,  2.7569e-02,  2.4402e-01,  1.9598e-01,  3.2345e-01,\n",
      "        -1.0675e-01,  9.9701e-02, -4.0205e-01, -1.0490e-01, -3.2857e-02,\n",
      "         1.1246e-01,  2.2799e-01, -3.1892e-01,  8.9102e-02, -2.7963e-01,\n",
      "        -3.2043e-01, -2.4213e-01, -7.1159e-01, -3.3415e-01, -1.4913e-02,\n",
      "        -9.8752e-01,  9.6680e-02,  1.4652e-01, -3.1112e-03,  9.5518e-02,\n",
      "        -2.0102e-02,  1.4009e+00,  2.8254e-01,  2.4854e-01, -2.6513e+00,\n",
      "         4.0125e-02, -3.1852e-01, -2.2552e-01,  5.0504e-01, -5.0386e-04,\n",
      "         3.5122e-01,  1.6179e-01,  4.9594e-01,  2.2691e-01,  7.4877e-02,\n",
      "         1.3729e-01, -2.6631e-01, -2.7150e-01, -1.8439e-01, -1.6404e-01,\n",
      "         1.0514e-02,  8.2247e-02, -1.0797e-01, -3.6018e-01, -2.9511e-01,\n",
      "        -3.6760e-01, -5.5279e-02, -4.0585e-01,  5.0557e-01,  9.3501e-01,\n",
      "         2.3752e-02,  1.7017e+00,  3.2012e-01,  4.7110e-03,  2.2390e-01,\n",
      "         1.4558e+00, -1.9728e-01,  1.7204e-01, -1.0011e-01,  2.5791e-02,\n",
      "        -6.6204e-02,  2.0974e-01, -6.3089e-02, -2.3667e-01, -2.2077e-01,\n",
      "        -2.0389e-01, -2.4854e-01,  2.6888e-01, -3.2500e-01, -4.2833e-01,\n",
      "        -2.8493e-01, -3.7335e-01,  3.3412e-01, -3.6080e-02, -1.8771e-01,\n",
      "        -7.4629e-02, -2.2938e-01, -1.2215e-01, -8.4950e-02,  1.9753e-01,\n",
      "         1.5035e+00,  1.6389e-01, -2.1843e-01,  5.3885e-01,  2.0972e-01,\n",
      "         1.3458e-01,  2.5675e-02,  7.5691e-02, -2.2233e-01, -3.4812e-01,\n",
      "        -5.0224e-01, -1.5370e-01,  1.9785e-01,  2.1220e-01,  2.2575e-01,\n",
      "        -8.8067e-02, -3.7519e-01,  2.9950e-01, -2.6255e-01, -1.5820e-02,\n",
      "        -2.4190e-01,  2.4495e-01, -1.4136e-01, -7.0102e-02,  1.5291e-01,\n",
      "        -9.1310e-02,  1.8918e-01,  2.8063e-01, -2.3391e-02, -8.0615e-02,\n",
      "         2.8979e-01,  1.0785e-01,  6.7327e-01, -4.8421e-02,  2.1317e-01,\n",
      "         5.6853e-02,  1.4203e-01,  2.5452e-01, -1.2208e-01,  1.1903e-01,\n",
      "         2.9845e-01,  1.8426e-01, -4.7766e-02, -2.8903e-01, -5.0162e-02,\n",
      "         5.0041e-01,  7.3264e-02, -9.3446e-02,  6.8084e-02,  5.9479e-01,\n",
      "        -6.1606e-02, -3.0160e-02,  8.8875e-01,  1.6161e-02,  2.5549e-01,\n",
      "         5.0033e-02,  2.4971e-01, -2.2907e-01,  4.8255e-01, -1.9999e+00,\n",
      "        -6.1097e-02,  1.1972e-01, -6.1656e-02, -3.1191e-01, -2.7626e-01,\n",
      "        -2.4046e-01,  1.1110e-01,  3.1134e-01, -1.0515e-01, -1.8156e-01,\n",
      "         1.0008e-01, -3.9676e-01,  2.0882e-01, -1.5823e+00, -2.5363e-02,\n",
      "        -2.0894e-01,  2.5724e-01, -3.8396e-01, -2.0240e-01, -1.5590e-01,\n",
      "         2.8246e-02, -7.8533e-02,  3.0916e-01, -4.6598e-02, -1.4644e-01,\n",
      "        -1.0970e-01,  9.6662e-01,  8.5621e-02,  3.9258e-01, -1.7076e-01,\n",
      "        -8.4942e-02,  3.1270e-03,  1.1809e-01, -4.8307e-02,  3.8276e-02,\n",
      "         1.4506e-01,  1.5558e-01, -1.5039e-01,  1.8149e-01, -1.9566e-02,\n",
      "        -4.4585e-02, -1.5940e-01, -3.1073e-01, -2.2559e-01,  3.4723e-03,\n",
      "        -2.5884e-01,  4.1708e-02,  1.4955e-01,  6.1888e-02, -6.5467e-02,\n",
      "         4.1612e-04, -1.3610e-01,  6.5405e-02,  1.9355e-01,  1.5466e-02,\n",
      "         1.5482e-01, -3.4185e-02,  3.7991e-01, -1.9813e-01,  1.3283e-01,\n",
      "         3.2539e-02, -2.0152e-01,  2.3094e-01,  1.9140e-01, -4.9553e-02,\n",
      "         6.6556e-03,  1.3492e-01, -1.9114e-01, -6.2233e-01, -6.2230e-01,\n",
      "        -8.2743e-02,  9.6988e-02,  1.2577e-01, -1.2103e-01, -4.2713e-02,\n",
      "        -4.4254e-01, -1.0114e-01,  1.6581e-01, -5.7012e-02, -1.5234e-01,\n",
      "         1.8616e-02, -1.1127e-01, -3.9327e-01, -1.0728e-01, -1.6209e+00,\n",
      "         9.1963e-02,  2.4490e-01, -8.9907e-02,  4.5106e-01,  2.6522e-01,\n",
      "        -6.3626e-03, -1.4228e-01,  2.3733e-02, -1.0218e-02, -2.4134e-01,\n",
      "        -3.8929e-01,  1.4449e-01,  1.8759e-01,  1.5055e-01,  1.0768e-01,\n",
      "         5.7867e-02, -1.7380e+00,  1.4707e-01,  3.2852e-01,  3.0718e-01,\n",
      "         2.6443e-01, -2.0025e-01,  1.9848e-01,  1.2229e-01, -7.8818e-02,\n",
      "         7.4550e-02,  2.6133e-01, -3.5963e-01,  1.4090e-02, -1.7053e-01,\n",
      "         6.1260e-03, -2.1407e-01,  3.9642e-03, -1.8596e-01,  6.3128e-02,\n",
      "         1.9560e-01, -1.9057e-01, -2.0586e-02,  1.6900e-01,  9.6205e-02,\n",
      "         3.3479e-02,  6.8197e-02, -8.4610e-02,  1.0462e-01,  1.7300e-01,\n",
      "         2.6195e-01,  5.5697e-02, -1.1563e-01, -3.4395e-01,  2.5220e-01,\n",
      "        -1.2301e-01,  1.5325e-01,  1.9225e-01, -1.0634e-01,  3.6847e-01,\n",
      "        -2.6048e-01,  6.6292e-02, -1.1383e-02, -2.5072e-01,  1.7366e-01,\n",
      "        -3.0343e-01, -2.1029e-01,  1.0132e-01, -7.4023e-02,  4.5125e-01,\n",
      "        -3.9692e-02,  3.0587e-01, -3.4324e-01, -4.0158e-01,  1.4401e-02,\n",
      "        -2.8310e-01,  3.6936e-01, -1.5028e-01, -2.4557e-01, -1.3667e-01,\n",
      "         9.3277e-02,  3.3141e-03, -1.2571e+00,  2.0752e-01,  3.7213e-01,\n",
      "        -1.6996e+00, -1.5869e-02, -2.9106e-02, -2.2561e-01, -3.8597e-02,\n",
      "        -6.7463e-02,  1.7014e-01, -3.8718e-01,  3.7265e-01,  2.8491e-02,\n",
      "         7.2793e-02, -3.0857e-01, -2.8430e-01,  2.2524e-01,  2.0574e-01,\n",
      "        -1.1836e-01,  1.8756e-01,  2.1217e-01,  3.3130e-01, -2.0681e-01,\n",
      "        -1.7211e+00,  7.4127e-02,  3.0686e-01,  2.2170e-01,  3.6608e-01,\n",
      "         1.5711e-01,  2.2508e-01,  1.6301e-01, -1.1923e-01, -4.7809e-02,\n",
      "         1.2986e-01, -6.3544e-01,  1.6519e-02, -2.3869e-01, -4.6152e-01,\n",
      "         3.2307e-02,  6.0489e-01,  4.9385e-03, -3.5510e-01,  2.7679e-01,\n",
      "        -7.0197e-02,  3.9840e-01, -1.8180e-01,  1.2617e-01,  7.6364e-02,\n",
      "        -2.6746e-01, -6.1173e-02, -1.2388e-01, -1.1274e+00, -1.8834e+00,\n",
      "         1.1560e+00, -3.4986e-01, -2.6801e-01, -5.4915e-04,  1.9124e+00,\n",
      "         3.2127e-02,  2.3596e-01, -5.8986e-03,  5.5572e-02,  1.1523e-01,\n",
      "         2.9394e+00,  2.1127e-01, -7.0968e-02,  1.3330e+00, -1.4936e-01,\n",
      "         7.6505e-02, -2.5428e-01, -1.2710e+00, -1.7324e+00, -5.4752e-02,\n",
      "        -4.1809e-01, -3.4292e-02,  6.9267e-02, -4.4702e-02, -1.6097e-01,\n",
      "         5.9707e-01, -8.1368e-02,  6.1452e-02, -1.0525e-01, -3.2538e-01,\n",
      "         7.0925e-02,  4.2220e-03,  2.4145e-03, -7.4062e-02,  4.9867e-01,\n",
      "         5.3334e-01,  8.0375e-02, -8.6044e-02,  2.0201e-01, -2.1088e-01,\n",
      "        -1.7914e+00,  2.2383e-01, -2.8591e-01,  2.6903e-01,  2.5222e-01,\n",
      "         1.0473e-01,  2.3397e-01,  4.5373e-02, -6.8843e-01,  1.4137e-01,\n",
      "         7.5989e-02,  3.0014e-01, -5.7641e-02,  1.3918e-01,  3.6217e-02,\n",
      "         5.1455e-01,  8.7784e-03,  1.0792e-01,  1.7822e+00, -1.2149e-01,\n",
      "        -1.8311e-01,  3.2075e-01,  7.5549e-02,  3.5324e-01, -2.9368e-01,\n",
      "         1.5668e-01,  2.9310e-02, -5.9035e-02,  3.2493e-01, -3.9433e-01,\n",
      "         5.9213e-02, -2.4474e-01,  7.1406e-02, -2.9831e-02,  2.2102e-02,\n",
      "         5.9649e-01, -9.8553e-02, -8.6978e-03, -9.6496e-01,  4.6884e-02,\n",
      "        -1.8342e-02,  1.8637e-01,  2.1527e-02, -3.5471e-01,  6.5687e-01,\n",
      "         4.7625e-01,  1.1447e-01, -2.7852e-02, -1.0932e-01, -4.5928e-01,\n",
      "        -2.1020e-01, -1.1677e-01,  6.7717e-02, -1.0000e-01,  1.7761e-01,\n",
      "        -1.9549e-01, -3.1074e-01, -1.4650e-01,  1.3374e+00,  8.4335e-02,\n",
      "         8.1341e-02,  9.8261e-01, -6.1368e-02, -3.4497e-01,  3.3050e-01,\n",
      "         2.4154e-01, -9.7968e-02,  8.4880e-02, -8.5132e-04,  1.5197e-02,\n",
      "        -5.4357e-02,  2.8248e-01, -3.5251e-01, -3.3237e-02,  1.4065e-01,\n",
      "         3.1314e-01,  3.1777e-01,  1.2210e-01,  1.9423e-01,  6.2747e-02,\n",
      "        -4.2637e-01, -2.3175e-01,  3.0893e-02,  2.7138e-02,  1.1094e-01,\n",
      "         3.6457e-01,  6.8648e-02,  7.6718e-02, -2.4160e-01, -9.7317e-02,\n",
      "         2.3566e-02,  7.5808e-02,  1.8659e-01, -2.4417e-01,  1.2730e-01,\n",
      "        -1.7313e-01, -2.0997e-01, -1.0517e-01, -4.7289e-01,  2.9566e-01,\n",
      "         4.5406e-02, -4.2102e-01,  3.0568e-01,  4.4347e-01,  1.3555e-01,\n",
      "         2.7082e-01, -2.3115e-01, -3.7554e-02,  1.3486e-01, -3.6536e-01,\n",
      "        -5.7206e-02,  2.7357e-01, -2.8770e-01,  6.0940e-02,  1.0658e-01,\n",
      "        -1.0222e-01,  1.9405e-02,  2.2733e-01, -6.9449e-02,  6.3672e-02,\n",
      "         5.1234e-01,  8.4173e-01,  1.1982e-01,  6.5755e-02, -4.0155e-01,\n",
      "         5.2774e-02, -2.8318e-01,  1.2400e-01, -1.7873e-01, -1.5785e-01,\n",
      "         5.2619e-02,  9.3177e-03,  2.1323e-01, -6.9808e-01,  4.0963e-01,\n",
      "         1.8804e-01, -5.3934e-02,  4.8268e-02,  1.5776e-02, -2.6004e-01,\n",
      "        -1.8568e+00, -3.8643e-01,  8.8235e-03,  6.9240e-02,  1.5022e-01,\n",
      "        -2.2416e-01,  7.5324e-01, -2.2330e-01, -2.4289e-02, -1.5414e-01,\n",
      "        -2.4606e-01,  3.7288e-01, -1.8232e-01, -8.2536e-02, -6.0095e-01,\n",
      "         1.6061e-01,  3.6422e-02,  2.2611e-01, -2.1432e-01,  7.1708e-02,\n",
      "        -1.2384e+00, -3.5852e-01, -2.2014e-01,  1.4307e-01, -1.4074e-01,\n",
      "        -1.7439e-01, -2.3780e-02,  1.8995e-01, -3.5891e-01,  1.8754e-01,\n",
      "         1.1169e-01, -3.4968e-01, -1.0830e-01, -1.0861e-01, -1.1898e-01,\n",
      "         6.1846e-01, -1.7223e-01,  1.6567e-02, -1.0378e-01,  2.3561e-01,\n",
      "        -2.5699e-02, -2.8287e-01, -2.7108e-01,  3.8700e-01,  3.0495e-01,\n",
      "         2.7775e-01,  2.0604e-02, -3.9275e-01,  6.0210e-02,  8.7302e-02,\n",
      "        -1.5283e+00, -1.7999e-01, -2.3755e-01,  1.5996e-01,  1.9787e-01,\n",
      "         1.3831e-01, -4.2936e-01, -1.1097e-02,  7.9145e-02, -1.7474e-01,\n",
      "        -3.3859e-01,  4.3210e-02,  5.2608e-02,  2.3329e-02,  3.7296e-02,\n",
      "        -1.5121e-01,  3.3934e-01,  3.6114e-01, -1.9324e-01, -1.1005e-01,\n",
      "         4.3963e-03, -1.4539e-01,  7.1829e-01,  3.2231e-02,  4.4448e-01,\n",
      "         3.4485e-01,  3.9877e-02,  4.5187e-02, -1.3156e-01,  1.7157e-01,\n",
      "         7.0164e-02, -9.3189e-01,  5.8303e-02, -9.7161e-02,  7.0078e-03,\n",
      "        -5.3653e-02, -1.9188e-01,  3.3162e-01,  3.3745e-01,  1.9542e-01,\n",
      "        -8.5812e-02,  4.4364e-01,  1.7596e+00,  2.6094e-01,  1.3094e-01,\n",
      "         1.1018e-01,  1.7920e-01, -1.1515e-01,  2.5182e-01, -2.4362e-01,\n",
      "         1.5615e+00,  2.0118e-01, -2.8853e-02, -3.0681e-01,  5.4162e-01,\n",
      "         3.7370e-01,  1.7376e+00, -1.5188e-01, -8.9351e-03,  1.4752e-01,\n",
      "         3.3810e-01,  4.0567e-02,  4.0923e-01, -3.3574e-01, -1.0794e-01,\n",
      "         9.2423e-02, -5.3121e-01, -1.1359e-01,  1.7517e-01,  4.7125e-01,\n",
      "        -1.5675e-01, -5.3425e-01,  7.2752e-02,  1.4367e-01, -3.5436e-01,\n",
      "         2.5004e-01, -2.0917e-01, -2.5489e-01, -4.6941e-02,  2.3767e-02,\n",
      "         5.5277e-02,  9.9213e-02, -1.0018e-01, -3.8157e-01, -5.4992e-01,\n",
      "        -2.2294e-01, -1.3198e-01,  5.7852e-01, -3.5800e-01,  3.5115e-01,\n",
      "         4.2515e-01, -4.2681e-02,  1.8919e-01,  4.0503e-01, -1.2616e-01,\n",
      "        -6.5322e-01, -8.6787e-01,  3.4107e-01,  7.0000e-01,  2.0792e-01,\n",
      "        -1.1445e-02,  1.7587e-01, -3.4712e-01])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Загрузка предобученной модели BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model     = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text = \"Токенизация – важный шаг в NLP.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Вектор CLS (первый токен)\n",
    "cls_embedding = outputs.last_hidden_state[:,0,:]   # shape: [1, hidden_dim]\n",
    "\n",
    "print(\"Размерность эмбеддинга:\", cls_embedding.shape)\n",
    "print(\"Эмбеддинг предложения:\\n\", cls_embedding[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
