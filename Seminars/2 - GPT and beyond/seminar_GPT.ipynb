{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwrjccfsem1U"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TDEtbNmJNP5i"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, list_datasets, Dataset\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Hla8m3wgjm"
   },
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ51auF9rop7"
   },
   "source": [
    "https://huggingface.co/docs/transformers/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ddvb-7swo6t4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Our device is {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1: интерпретируем attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам даны attention-скоры для последовательности ниже. Необходимо определить позицию токена, на который больше всего \"обращал внимание\" токен на позиции 4 во второй attention head на последнем слое модели *(все вышеупомянутые числа- индексы!)* . Ответом должен быть индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2', output_attentions=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  2068,  7586, 21831, 18045,   625,   262, 16931,  3290,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(\"The quick brown fox jumps over the lazy dog.\", return_tensors='pt')\n",
    "model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация](https://huggingface.co/docs/transformers/v4.45.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel.forward) к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR IMPLEMENTAION HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jstVDi-2Y-7d"
   },
   "source": [
    "# Как работает генерация с т.з. кода под капотом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G9nI6GwqNlSl"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "frkwEHCUVhIZ"
   },
   "outputs": [],
   "source": [
    "TEXT_INPUT = \"Парламент- это не место для\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zDqkoPR9VfPx"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(TEXT_INPUT, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ASX286Q2rb4G"
   },
   "outputs": [],
   "source": [
    "for k, v in inputs.items():\n",
    "  inputs[k] = v.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OEAJfqmyiqWq"
   },
   "outputs": [],
   "source": [
    "bare_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "bare_model.eval()\n",
    "bare_model.to(DEVICE)\n",
    "bare_outputs = bare_model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "slyM9NGEZfFg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HKT71AzAvbJ5"
   },
   "outputs": [],
   "source": [
    "last_hidden_states = bare_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBExneKSwz_J"
   },
   "source": [
    "Weight tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IQl0LxZjwqfK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lena-voita.github.io/resources/lectures/lang_models/practical/weight_tying_idea-min.png\" width=\"1900\" height=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://lena-voita.github.io/resources/lectures/lang_models/practical/weight_tying_idea-min.png\", width=1900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mezRRFrkjzTd"
   },
   "outputs": [],
   "source": [
    "logits = torch.matmul(\n",
    "    last_hidden_states[-1][-1],\n",
    "    bare_model.wte.weight.T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "C6tdRDgsj8W-"
   },
   "outputs": [],
   "source": [
    "bare_probas = F.softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wZLo69g5j9VC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12466, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(bare_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZSih2orj-ax"
   },
   "source": [
    "# Упрощённая генерация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tBcgTY12kV5D"
   },
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "llm_model.eval()\n",
    "llm_model.to(DEVICE)\n",
    "llm_outputs = llm_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-Uoilh3vnxMQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for param in llm_model.parameters(recurse=True):\n",
    "    n_params += param.numel()\n",
    "\n",
    "\n",
    "n_params = str(n_params)\n",
    "n_params = \",\".join(\n",
    "    [\n",
    "        n_params[i: i+3]\n",
    "        for i in range(\n",
    "            len(n_params) - 3, -1, -3\n",
    "        )\n",
    "    ][::-1]\n",
    ")\n",
    "print(f\"Number of parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DiWJ1qWXkk7H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12466, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(llm_outputs.logits[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0GcBrRvrtBRZ"
   },
   "outputs": [],
   "source": [
    "llm_probas = F.softmax(llm_outputs.logits[-1][-1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Uw1dNB-RtVW7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(bare_probas, llm_probas, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99_zi1CR6F4r"
   },
   "source": [
    "# Задание 2: написать fine-tuning для языковой модели под набор данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tEYC5MbGDmJ"
   },
   "source": [
    "Описание датасета можно найти тут: \\\n",
    "https://paperswithcode.com/dataset/rucos \\\n",
    "https://huggingface.co/datasets/RussianNLP/russian_super_glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wvVA4G8jbh_R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unicornred/python_env/torch_gpu/lib64/python3.13/site-packages/datasets/load.py:1429: FutureWarning: The repository for RussianNLP/russian_super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/RussianNLP/russian_super_glue\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1a9e7ba5a54a449666c349cbd69317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567f5d76e04a47e5ac18c88b8a7fc346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446dfa453023490988bafc9fd4b256a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/56.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a4574bc3445c2aa7855d1943d54a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/72193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6220436487344a0987c9ff7a57d2a717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab454c6ee032440c8e208603588539f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"RussianNLP/russian_super_glue\", name='rucos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LtQ8I6d_KHBD"
   },
   "outputs": [],
   "source": [
    "RE_BAD_PATTERNS = re.compile(\"(@[a-z]+|\\n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-s2lrzCB6DIz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Международная группа исследователей из России и США пришла к выводу, что города, находящиеся в зоне вечной мерзлоты в Восточной Сибири, могут разрушиться к 2050 году. Причиной является оттаивание грунтов из-за глобального потепления. Об этом сообщает издание The Siberian Times. Согласно оценке исследователей, через 35 лет несущая способность пород, в которых содержится вечная мерзлота, может уменьшиться на 75-95 процентов. Это приведет к деформации и обрушению как жилых, так и промышленных сооружений. Самые быстрые изменения коснутся городов Анадырь и Салехард — здания могут потерять устойчивость к середине 2020-х годов.\n",
      "@highlight\n",
      "Из вечной мерзлоты извлекли древний мозг щенка\n",
      "@highlight\n",
      "Гигантские воронки могут появиться на севере России\n",
      "@highlight\n",
      "Найдено объяснение сибирской аномалии\n",
      "********************\n",
      "Международная группа исследователей из России и США пришла к выводу, что города, находящиеся в зоне вечной мерзлоты в Восточной Сибири, могут разрушиться к 2050 году. Причиной является оттаивание грунтов из-за глобального потепления. Об этом сообщает издание The Siberian Times. Согласно оценке исследователей, через 35 лет несущая способность пород, в которых содержится вечная мерзлота, может уменьшиться на 75-95 процентов. Это приведет к деформации и обрушению как жилых, так и промышленных сооружений. Самые быстрые изменения коснутся городов Анадырь и Салехард — здания могут потерять устойчивость к середине 2020-х годов.   Из вечной мерзлоты извлекли древний мозг щенка   Гигантские воронки могут появиться на севере России   Найдено объяснение сибирской аномалии\n"
     ]
    }
   ],
   "source": [
    "random_idx_from_train = random.randint(0, len(dataset['train']) - 1) # 66411\n",
    "\n",
    "random_object = dataset['train'][random_idx_from_train]['passage']\n",
    "\n",
    "filtered_random_object = RE_BAD_PATTERNS.sub(\" \", random_object)\n",
    "print(random_object)\n",
    "print(\"*\" * 20)\n",
    "print(filtered_random_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "unuLI-A_Itji"
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': \"<|endoftext|>\"})\n",
    "\n",
    "def texts_to_batch(texts: List[str]) -> torch.Tensor:\n",
    "    clean_texts = [\n",
    "        RE_BAD_PATTERNS.sub(\" \", _[\"passage\"]) for _ in texts\n",
    "    ]\n",
    "    tokenized_texts = tokenizer(\n",
    "        text=clean_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "      )\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HKyQJ5AvqXz-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=texts_to_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rnG7TkFTY2K7"
   },
   "outputs": [],
   "source": [
    "N_ITERATIONS = 1000\n",
    "\n",
    "inputs = tokenizer(\"В прошлый четверг президенты Казахстана и России\", return_tensors=\"pt\")\n",
    "for k, v in inputs.items():\n",
    "    inputs[k] = v.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cBNB57wmZC08"
   },
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TXwBeXHkozD8"
   },
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "llm_model.eval()\n",
    "llm_model.to(DEVICE)\n",
    "llm_outputs = llm_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2fF7wjuYyHb"
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=llm_model.parameters(), lr=1e-6\n",
    ")\n",
    "\n",
    "cur_iteration = 0\n",
    "for batch in train_dl:\n",
    "    if cur_iteration == N_ITERATIONS:\n",
    "        break\n",
    "\n",
    "    llm_model.train()\n",
    "    \"\"\"\n",
    "    YOUR IMPLEMENTAION HERE\n",
    "    \"\"\"\n",
    "    print(f\"Loss value: {loss_value.item()}\")\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    llm_model.eval()\n",
    "    for n_beams in range(2, 5):\n",
    "        beam_output = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE, num_beams=n_beams)\n",
    "        print(f\"Beam size={n_beams}\")\n",
    "        print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
    "        print()\n",
    "\n",
    "    print(\"*\" * 20)\n",
    "    cur_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ3gKx_SN8Td"
   },
   "source": [
    "# Задание 3: написать greedy search, сравнить результаты с имплементацией от transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5f0tj1ROp2W"
   },
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdVqCO6YtNlC"
   },
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "llm_model.eval()\n",
    "llm_model.to(DEVICE)\n",
    "llm_outputs = llm_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pir2YSvgOtIZ"
   },
   "outputs": [],
   "source": [
    "def convert_to_expected_input(input_ids, attention_mask):\n",
    "    input_ids = torch.tensor(input_ids, device=DEVICE)\n",
    "    attention_mask = torch.tensor(attention_mask, device=DEVICE)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c7URz8aOv39"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask = inputs[\"input_ids\"].tolist(), inputs[\"attention_mask\"].tolist()\n",
    "\"\"\"\n",
    "input_ids должно содержать нагенерированные токены\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(OUTPUT_SIZE):\n",
    "    \"\"\"\n",
    "    YOUR IMPLEMENTAION HERE\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eseVteQSSVxK"
   },
   "outputs": [],
   "source": [
    "llm_predictions = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36G_8mrjVCnK"
   },
   "source": [
    "Ниже проверяем, что наивная имплементация совпадает с ожидаемой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nR81p70TgJx"
   },
   "outputs": [],
   "source": [
    "assert input_ids[-1][-OUTPUT_SIZE:] == llm_predictions[-1][-OUTPUT_SIZE:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQOUB10sTisH"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(llm_predictions[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMz7pt76auXd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNCaHHzjLK0r89lnHde8MVr",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
